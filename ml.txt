


## ✅ **Practical 2: Python Libraries for ML**

### a) Math, NumPy, SciPy
```python
import math
print(math.ceil(1.4))         # Rounds up
print(math.floor(1.4))        # Rounds down
print(math.factorial(9))      # Factorial
print(math.sqrt(64))          # Square root
print(math.isnan(56))         # Is NaN?

import numpy as np

# 1D Array
arr = np.array([1, 2, 3, 4, 5])
print(arr)

# 2D Array
arr = np.array([[1, 2, 3], [4, 5, 6]])
print(arr)
print(arr.ndim)

# Range and Reshape
print(np.arange(3))
print(np.arange(12).reshape((3, 4)))
print(np.arange(24).reshape((2, 3, 4)))

print(type(arr))
print(arr.shape)

# Arithmetic
a = np.array([20, 30, 40, 50, 60])
b = np.arange(5)
c = a - b
print(c)
```

---

### b) Pandas and Matplotlib
```python
import pandas as pd

# Series from dict
calories = {"day1": 420, "day2": 380, "day3": 390}
myvar = pd.Series(calories)
print(myvar)

# DataFrame
data = {
  "calories": [420, 380, 390],
  "duration": [50, 40, 45]
}
df = pd.DataFrame(data)
print(df)
print(df.loc[0])

# Custom index
df = pd.DataFrame(data, index=["day1", "day2", "day3"])
print(df)

# Reading a CSV (upload 'data.csv' beforehand)
# df = pd.read_csv('data.csv')
# print(df.to_string())
```

```python
import matplotlib.pyplot as plt
import numpy as np

# Simple Line
xpoints = np.array([0, 6])
ypoints = np.array([0, 250])
plt.plot(xpoints, ypoints, color='r', ls=":")
plt.show()

# Markers only
plt.plot(xpoints, ypoints, 'o')
plt.show()


Perfect! Here's the full code for all the **CSL604 Machine Learning Lab Experiments (1–9)** based on your manual:

---

## ✅ **Practical 3: Simple Linear Regression**
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Sample Data
x = np.array([1, 2, 3, 4, 5]).reshape((-1, 1))
y = np.array([2, 4, 5, 4, 5])

# Split data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

# Model
model = LinearRegression()
model.fit(x_train, y_train)

# Predict
y_pred = model.predict(x_test)

# Plot
plt.scatter(x, y, color='blue')
plt.plot(x_test, y_pred, color='red')
plt.title('Simple Linear Regression')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
```

---

## ✅ **Practical 4: Logistic Regression**
```python
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Sample Data
df = pd.DataFrame({
    'Age': [22, 25, 47, 52, 46, 56],
    'Salary': [15000, 29000, 48000, 60000, 52000, 61000],
    'Purchased': [0, 0, 1, 1, 1, 1]
})

X = df[['Age', 'Salary']]
y = df['Purchased']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
```

---

## ✅ **Practical 5: Support Vector Machine with Kernels**
```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

iris = datasets.load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Linear Kernel
linear = SVC(kernel='linear')
linear.fit(X_train, y_train)
print("Linear Kernel Accuracy:", accuracy_score(y_test, linear.predict(X_test)))

# Polynomial Kernel
poly = SVC(kernel='poly', degree=3)
poly.fit(X_train, y_train)
print("Polynomial Kernel Accuracy:", accuracy_score(y_test, poly.predict(X_test)))

# RBF Kernel
rbf = SVC(kernel='rbf')
rbf.fit(X_train, y_train)
print("RBF Kernel Accuracy:", accuracy_score(y_test, rbf.predict(X_test)))
```

---

## ✅ **Practical 6: Principal Component Analysis (PCA)**
```python
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

iris = load_iris()
X = iris.data
y = iris.target

pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y)
plt.title('PCA on Iris Dataset')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()
```

---

## ✅ **Practical 7: Hebbian Learning Rule**
```python
import numpy as np

# Input vectors
X = np.array([[-1, -1, 1],
              [-1, 1, 1],
              [1, -1, 1],
              [1, 1, 1]])

# Target output
Y = np.array([-1, -1, -1, 1])

# Initialize weights
w = np.zeros(3)

# Hebbian learning
for i in range(len(X)):
    w += X[i] * Y[i]
    print(f"After iteration {i+1}, Weights: {w}")

print("\nFinal weight matrix:", w)
```

---

## ✅ **Practical 8: Single Layer Perceptron**
```python
import numpy as np

# Input (AND Gate)
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])
y = np.array([0, 0, 0, 1])

# Parameters
w = np.zeros(2)
b = 0
lr = 0.1
epochs = 10

# Activation function
def activation(x):
    return 1 if x >= 0 else 0

for epoch in range(epochs):
    for i in range(len(X)):
        x_i = X[i]
        y_hat = activation(np.dot(x_i, w) + b)
        error = y[i] - y_hat
        w += lr * error * x_i
        b += lr * error

print("Weights:", w)
print("Bias:", b)
```

---

## ✅ **Practical 9: Backpropagation Algorithm**
```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Input data
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

# Expected output
y = np.array([[0], [1], [1], [0]])

# Initialize weights
inputLayerNeurons = 2
hiddenLayerNeurons = 2
outputNeurons = 1

# Random weights and bias
hidden_weights = np.random.uniform(size=(inputLayerNeurons, hiddenLayerNeurons))
hidden_bias = np.random.uniform(size=(1, hiddenLayerNeurons))
output_weights = np.random.uniform(size=(hiddenLayerNeurons, outputNeurons))
output_bias = np.random.uniform(size=(1, outputNeurons))

# Training loop
for _ in range(10000):
    # Forward Propagation
    hidden_layer_input = np.dot(X, hidden_weights) + hidden_bias
    hidden_layer_output = sigmoid(hidden_layer_input)

    output_layer_input = np.dot(hidden_layer_output, output_weights) + output_bias
    output = sigmoid(output_layer_input)

    # Backpropagation
    error = y - output
    d_output = error * sigmoid_derivative(output)

    error_hidden_layer = d_output.dot(output_weights.T)
    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)

    # Update Weights and Biases
    output_weights += hidden_layer_output.T.dot(d_output)
    output_bias += np.sum(d_output, axis=0, keepdims=True)
    hidden_weights += X.T.dot(d_hidden_layer)
    hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True)

print("Final output after training:\n", output)
```

